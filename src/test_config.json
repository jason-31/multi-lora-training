{
  "batch_size": 8,
  "seq_len": 16,
  "d_model": 128,
  "num_heads": 4,
  "d_ff": 512,
  "vocab_size": 1000,
  "context_length": 64,
  "num_layers": 2,
  "theta": 10000.0,
  "lora_rank": 8,
  "lora_alpha": 16
}
